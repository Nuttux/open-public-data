#!/usr/bin/env python3
"""
Script de fusion des investissements localis√©s.

Ce script fusionne deux sources de donn√©es pour les investissements:
1. PDF "Investissements Localis√©s" (source principale - projets d√©taill√©s avec adresses)
2. BigQuery OpenData (compl√©ment - gros projets iconiques manquants)

M√âTHODOLOGIE DE FUSION:
-----------------------
Le PDF IL contient ~90% des projets localis√©s avec des descriptions d√©taill√©es
incluant souvent des adresses pr√©cises. Cependant, il manque certains gros projets
iconiques (Philharmonie, Th√©√¢tre de la Ville, etc.) qui sont dans BigQuery.

R√àGLE D'AJOUT DEPUIS BIGQUERY:
- A un arrondissement OU est un lieu iconique connu
- ET montant > 500k‚Ç¨ (projets significatifs)
- ET n'est PAS une subvention g√©n√©rique "logement social"
- ET n'est PAS d√©j√† pr√©sent dans le PDF

LIEUX ICONIQUES (ajout√©s m√™me si cat√©goris√©s diff√©remment):
- Philharmonie de Paris
- Th√©√¢tre de la Ville
- Op√©ra (Bastille, Garnier)
- Tour Eiffel
- Notre-Dame
- H√¥tel de Ville

Usage:
    python merge_investments.py --year 2022
    python merge_investments.py --all

Output:
    website/public/data/map/investissements_complet_{year}.json
"""

import argparse
import json
import re
from datetime import datetime
from pathlib import Path
from typing import Optional

# =============================================================================
# Configuration
# =============================================================================

PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent.parent
DATA_DIR = PROJECT_ROOT / "website" / "public" / "data" / "map"

# Seuil minimum pour ajouter un projet depuis BigQuery
MIN_AMOUNT_BQ = 500_000  # 500k‚Ç¨

# Lieux iconiques parisiens (toujours inclus m√™me si nom contient "SUB")
LIEUX_ICONIQUES = [
    'philharmonie',
    'theatre de la ville',
    'opera bastille',
    'opera garnier',
    'tour eiffel',
    'notre-dame',
    'notre dame',
    'hotel de ville',
    'palais de tokyo',
    'petit palais',
    'grand palais',
]

# Patterns √† exclure (subventions g√©n√©riques non localisables)
EXCLUSION_PATTERNS = [
    r'sub.*logement\s*soci',      # SUB EQUIPEMENT LOGEMENT SOCIAL
    r'subvention.*logement',       # SUBVENTION AU TITRE DU LOGEMENT
]


# =============================================================================
# Fonctions utilitaires
# =============================================================================

def is_iconic_location(name: str) -> bool:
    """V√©rifie si le nom contient un lieu iconique parisien."""
    name_lower = name.lower()
    return any(lieu in name_lower for lieu in LIEUX_ICONIQUES)


def is_excluded(name: str) -> bool:
    """V√©rifie si le projet doit √™tre exclu (subvention g√©n√©rique)."""
    name_lower = name.lower()
    # Ne pas exclure les lieux iconiques m√™me s'ils contiennent "SUB"
    if is_iconic_location(name):
        return False
    return any(re.search(pattern, name_lower) for pattern in EXCLUSION_PATTERNS)


def should_add_from_bq(name: str, arrondissement: Optional[int], montant: float) -> tuple[bool, str]:
    """
    D√©termine si un projet BigQuery doit √™tre ajout√©.
    
    Returns:
        tuple: (should_add, reason)
    """
    # V√©rifier le montant minimum
    if montant < MIN_AMOUNT_BQ:
        return False, "MONTANT_TROP_FAIBLE"
    
    # Exclure les subventions g√©n√©riques
    if is_excluded(name):
        return False, "SUBVENTION_GENERIQUE"
    
    # Inclure si lieu iconique
    if is_iconic_location(name):
        return True, "LIEU_ICONIQUE"
    
    # Inclure si a un arrondissement
    if arrondissement:
        return True, "AVEC_ARRONDISSEMENT"
    
    # Exclure le reste (citywide sans localisation)
    return False, "CITYWIDE_GENERIQUE"


def normalize_name(name: str) -> str:
    """Normalise un nom pour la comparaison."""
    # Minuscules, supprime accents et caract√®res sp√©ciaux
    name = name.lower()
    name = re.sub(r'[√©√®√™√´]', 'e', name)
    name = re.sub(r'[√†√¢√§]', 'a', name)
    name = re.sub(r'[√π√ª√º]', 'u', name)
    name = re.sub(r'[√Æ√Ø]', 'i', name)
    name = re.sub(r'[√¥√∂]', 'o', name)
    name = re.sub(r'[√ßc]', 'c', name)
    name = re.sub(r'[^a-z0-9\s]', ' ', name)
    name = re.sub(r'\s+', ' ', name).strip()
    return name


def is_similar_project(pdf_name: str, bq_name: str) -> bool:
    """
    V√©rifie si deux projets sont similaires (d√©j√† pr√©sent dans PDF).
    
    Utilise une correspondance par mots-cl√©s significatifs.
    """
    pdf_normalized = normalize_name(pdf_name)
    bq_normalized = normalize_name(bq_name)
    
    # Extraire les mots significatifs (>3 caract√®res)
    bq_keywords = [w for w in bq_normalized.split() if len(w) > 3][:3]
    
    if not bq_keywords:
        return False
    
    # Tous les mots-cl√©s doivent √™tre pr√©sents
    return all(kw in pdf_normalized for kw in bq_keywords)


# =============================================================================
# Fusion
# =============================================================================

def load_pdf_data(year: int) -> Optional[dict]:
    """Charge les donn√©es PDF pour une ann√©e."""
    # Essayer d'abord investissements_localises, sinon investissements
    for pattern in [f'investissements_localises_{year}.json', f'investissements_{year}.json']:
        path = DATA_DIR / pattern
        if path.exists():
            with open(path, encoding='utf-8') as f:
                data = json.load(f)
            return {
                'source': 'PDF' if 'localises' in pattern else 'BQ_ONLY',
                'path': str(path),
                'data': data.get('data', []),
                'stats': data.get('stats', {}),
            }
    return None


def load_bq_data(year: int) -> Optional[dict]:
    """Charge les donn√©es BigQuery pour une ann√©e."""
    path = DATA_DIR / f'investissements_{year}.json'
    if path.exists():
        with open(path, encoding='utf-8') as f:
            data = json.load(f)
        return {
            'source': 'BQ',
            'path': str(path),
            'data': data.get('data', []),
        }
    return None


def merge_year(year: int, dry_run: bool = False) -> dict:
    """
    Fusionne les donn√©es PDF et BigQuery pour une ann√©e.
    
    Returns:
        dict avec les statistiques et les donn√©es fusionn√©es
    """
    print(f"\n{'='*60}")
    print(f"üìä Fusion {year}")
    print(f"{'='*60}")
    
    # Charger les donn√©es
    pdf_result = load_pdf_data(year)
    bq_result = load_bq_data(year)
    
    if not pdf_result:
        print(f"  ‚ö†Ô∏è Pas de donn√©es PDF pour {year}")
        return {'year': year, 'status': 'NO_PDF_DATA'}
    
    pdf_data = pdf_result['data']
    pdf_source = pdf_result['source']
    
    print(f"  üìÑ PDF ({pdf_source}): {len(pdf_data)} projets")
    
    # Si pas de BQ, retourner juste le PDF
    if not bq_result or pdf_source == 'BQ_ONLY':
        print(f"  ‚ÑπÔ∏è Pas de donn√©es BQ √† fusionner")
        merged_data = pdf_data
        added_from_bq = []
    else:
        bq_data = bq_result['data']
        print(f"  üî∑ BQ: {len(bq_data)} lignes")
        
        # Agr√©ger BQ par nom (d√©dupliquer)
        bq_aggregated = {}
        for p in bq_data:
            name = p.get('apTexte', '')
            if name not in bq_aggregated:
                bq_aggregated[name] = {
                    'montant': 0,
                    'arrondissement': p.get('arrondissement'),
                    'missionTexte': p.get('missionTexte'),
                    'thematique': p.get('thematique'),
                }
            bq_aggregated[name]['montant'] += p['montant']
        
        print(f"  üî∑ BQ unique: {len(bq_aggregated)} projets")
        
        # Identifier les projets BQ √† ajouter
        added_from_bq = []
        skipped = {'MONTANT_TROP_FAIBLE': 0, 'SUBVENTION_GENERIQUE': 0, 
                   'CITYWIDE_GENERIQUE': 0, 'DEJA_DANS_PDF': 0}
        
        for name, bq_info in bq_aggregated.items():
            # V√©rifier si d√©j√† dans PDF
            already_in_pdf = any(
                is_similar_project(p.get('nom_projet', ''), name)
                for p in pdf_data
            )
            
            if already_in_pdf:
                skipped['DEJA_DANS_PDF'] += 1
                continue
            
            # V√©rifier si doit √™tre ajout√©
            should_add, reason = should_add_from_bq(
                name, 
                bq_info['arrondissement'], 
                bq_info['montant']
            )
            
            if should_add:
                added_from_bq.append({
                    'nom_projet': name,
                    'montant': bq_info['montant'],
                    'arrondissement': bq_info['arrondissement'] or 0,
                    'source': 'BigQuery',
                    'reason': reason,
                    'thematique': bq_info.get('thematique', ''),
                    'type_ap': 'grands_projets',
                    'confidence': 0.9,
                })
            else:
                skipped[reason] = skipped.get(reason, 0) + 1
        
        # Fusionner
        merged_data = pdf_data.copy()
        for p in merged_data:
            p['source'] = 'PDF'
        merged_data.extend(added_from_bq)
        
        print(f"\n  ‚úÖ Ajout√©s depuis BQ: {len(added_from_bq)} projets")
        for reason, count in skipped.items():
            if count > 0:
                print(f"  ‚ùå Ignor√©s ({reason}): {count}")
    
    # Calculer les totaux
    pdf_total = sum(p['montant'] for p in pdf_data)
    merged_total = sum(p['montant'] for p in merged_data)
    added_total = sum(p['montant'] for p in added_from_bq) if added_from_bq else 0
    
    print(f"\n  üìä R√©sum√©:")
    print(f"     PDF original: {len(pdf_data)} projets, {pdf_total/1e6:.2f} M‚Ç¨")
    print(f"     Ajout√©s BQ:   {len(added_from_bq)} projets, {added_total/1e6:.2f} M‚Ç¨")
    print(f"     TOTAL:        {len(merged_data)} projets, {merged_total/1e6:.2f} M‚Ç¨")
    
    # Afficher les projets ajout√©s
    if added_from_bq:
        print(f"\n  üìã Projets ajout√©s depuis BQ:")
        added_sorted = sorted(added_from_bq, key=lambda x: -x['montant'])
        for p in added_sorted[:10]:
            arr = p['arrondissement'] if p['arrondissement'] else '?'
            print(f"     {p['montant']/1e6:6.2f} M‚Ç¨ | Arr {arr:>2} | {p['nom_projet'][:40]}")
    
    # Sauvegarder si pas dry_run
    if not dry_run:
        output_path = DATA_DIR / f'investissements_complet_{year}.json'
        output_data = {
            'year': year,
            'source': 'Fusion PDF + BigQuery',
            'methodology': 'PDF Investissements Localis√©s + Gros projets BQ (>500k‚Ç¨, localisables)',
            'generated_at': datetime.now().isoformat(),
            'stats': {
                'pdf_projets': len(pdf_data),
                'pdf_total': pdf_total,
                'bq_added': len(added_from_bq),
                'bq_added_total': added_total,
                'total_projets': len(merged_data),
                'total_montant': merged_total,
            },
            'data': merged_data,
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"\n  ‚úì Sauvegard√©: {output_path}")
    
    return {
        'year': year,
        'status': 'OK',
        'pdf_projets': len(pdf_data),
        'pdf_total': pdf_total,
        'bq_added': len(added_from_bq),
        'bq_added_total': added_total,
        'total_projets': len(merged_data),
        'total_montant': merged_total,
        'added_projects': added_from_bq,
    }


def update_index(years: list[int]):
    """Met √† jour l'index des investissements complets."""
    index_path = DATA_DIR / 'investissements_complet_index.json'
    
    year_stats = {}
    for year in years:
        data_path = DATA_DIR / f'investissements_complet_{year}.json'
        if data_path.exists():
            with open(data_path, encoding='utf-8') as f:
                data = json.load(f)
            year_stats[year] = data.get('stats', {})
    
    index = {
        'availableYears': sorted(years, reverse=True),
        'source': 'Fusion PDF Investissements Localis√©s + BigQuery',
        'lastUpdate': datetime.now().isoformat(),
        'yearStats': year_stats,
    }
    
    with open(index_path, 'w', encoding='utf-8') as f:
        json.dump(index, f, ensure_ascii=False, indent=2)
    
    print(f"\n‚úì Index mis √† jour: {index_path}")


# =============================================================================
# Main
# =============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="Fusion des investissements PDF + BigQuery",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )
    parser.add_argument(
        '--year', type=int,
        help="Ann√©e sp√©cifique √† traiter"
    )
    parser.add_argument(
        '--all', action='store_true',
        help="Traiter toutes les ann√©es disponibles"
    )
    parser.add_argument(
        '--dry-run', action='store_true',
        help="Afficher sans sauvegarder"
    )
    
    args = parser.parse_args()
    
    print("\n" + "="*60)
    print("üîÄ Fusion Investissements PDF + BigQuery")
    print("="*60)
    
    # D√©terminer les ann√©es √† traiter
    if args.year:
        years = [args.year]
    elif args.all:
        # Trouver toutes les ann√©es avec des donn√©es PDF
        years = []
        for f in DATA_DIR.glob('investissements_localises_*.json'):
            match = re.search(r'(\d{4})', f.name)
            if match:
                years.append(int(match.group(1)))
        # Ajouter aussi les ann√©es avec seulement BQ
        for f in DATA_DIR.glob('investissements_*.json'):
            if 'localises' not in f.name and 'complet' not in f.name and 'index' not in f.name:
                match = re.search(r'(\d{4})', f.name)
                if match:
                    year = int(match.group(1))
                    if year not in years:
                        years.append(year)
        years = sorted(years)
    else:
        print("‚ùå Sp√©cifiez --year YYYY ou --all")
        return
    
    print(f"Ann√©es √† traiter: {years}")
    
    # Fusionner chaque ann√©e
    results = []
    for year in years:
        result = merge_year(year, dry_run=args.dry_run)
        results.append(result)
    
    # Mettre √† jour l'index
    if not args.dry_run:
        successful_years = [r['year'] for r in results if r.get('status') == 'OK']
        if successful_years:
            update_index(successful_years)
    
    # R√©sum√© final
    print("\n" + "="*60)
    print("üìä R√âSUM√â FINAL")
    print("="*60)
    
    total_projets = sum(r.get('total_projets', 0) for r in results if r.get('status') == 'OK')
    total_montant = sum(r.get('total_montant', 0) for r in results if r.get('status') == 'OK')
    total_added = sum(r.get('bq_added', 0) for r in results if r.get('status') == 'OK')
    
    print(f"  Ann√©es trait√©es: {len([r for r in results if r.get('status') == 'OK'])}")
    print(f"  Total projets:   {total_projets}")
    print(f"  Total montant:   {total_montant/1e6:.2f} M‚Ç¨")
    print(f"  Ajout√©s de BQ:   {total_added}")
    
    if not args.dry_run:
        print("\n‚úÖ Fusion termin√©e!")
        print("\nProchaine √©tape: G√©olocaliser les projets")
        print("  python scripts/geocode_investments.py --all")


if __name__ == "__main__":
    main()
