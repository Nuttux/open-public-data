#!/usr/bin/env python3
"""
Synchronisation des donnÃ©es Paris Open Data vers BigQuery.

Ce script tÃ©lÃ©charge les donnÃ©es fraÃ®ches depuis l'API Paris Open Data
et les charge dans BigQuery pour traitement par dbt.

Usage:
    # Synchroniser toutes les sources
    python scripts/sync_opendata.py
    
    # Synchroniser une source spÃ©cifique
    python scripts/sync_opendata.py --source budget_execute
    
    # Lister les sources disponibles
    python scripts/sync_opendata.py --list

Datasets synchronisÃ©s (7 sources essentielles):
    - budget_principal: Comptes Administratifs - Budget Principal (exÃ©cutÃ©, SOURCE DE VÃ‰RITÃ‰)
    - budget_vote: Budgets VotÃ©s - Budget Principal (prÃ©visionnel, 2019-2026)
    - ap_projets: Comptes Administratifs - AP ExÃ©cutÃ©s (projets nommÃ©s)
    - subventions: Annexe CA - Subventions versÃ©es (tous bÃ©nÃ©ficiaires)
    - associations: Subventions associations (avec SIRET pour gÃ©oloc)
    - logements_sociaux: Logements sociaux (dÃ©jÃ  gÃ©olocalisÃ©s)
    - marches_publics: MarchÃ©s publics (contexte, non sommable)
    
Convention de nommage RAW:
    Table BigQuery = dataset_id OpenData en snake_case

Output:
    - Tables BigQuery dans le dataset 'raw'
"""

import argparse
import io
import json
import os
import re
import sys
import unicodedata
from datetime import datetime
from pathlib import Path
from typing import Optional

import pandas as pd
import requests
from google.cloud import bigquery

# =============================================================================
# Configuration
# =============================================================================

# BigQuery
PROJECT_ID = "open-data-france-484717"
DATASET_ID = "raw"

# API Paris Open Data
OPENDATA_API = "https://opendata.paris.fr/api/explore/v2.1"

# =============================================================================
# Mapping des sources vers les datasets Paris Open Data
#
# CONVENTION DE NOMMAGE RAW:
#   Table BigQuery = dataset_id OpenData en snake_case (aucune abrÃ©viation)
#   Exemple: "comptes-administratifs-budgets-principaux-a-partir-de-2019-m57-ville-departement"
#         â†’ raw.comptes_administratifs_budgets_principaux_a_partir_de_2019_m57_ville_departement
#
# ARCHITECTURE (7 sources essentielles):
# - budget_principal: Budget exÃ©cutÃ© (SOURCE DE VÃ‰RITÃ‰ macro, CA)
# - budget_vote: Budget votÃ© (prÃ©visionnel, BP, 2019-2026)
# - ap_projets: AP ExÃ©cutÃ©s (projets nommÃ©s, ~10% du budget I)
# - subventions: Annexe CA subventions (tous bÃ©nÃ©ficiaires)
# - associations: Subventions associations (avec SIRET pour gÃ©oloc)
# - logements_sociaux: Programmes de logements (dÃ©jÃ  gÃ©olocalisÃ©s)
# - marches_publics: MarchÃ©s publics (contexte, non sommable)
# =============================================================================
DATASETS = {
    # =========================================================================
    # BUDGET PRINCIPAL - Source de vÃ©ritÃ© pour les totaux
    # =========================================================================
    "budget_principal": {
        "dataset_id": "comptes-administratifs-budgets-principaux-a-partir-de-2019-m57-ville-departement",
        "description": "Comptes Administratifs - Budget Principal (ExÃ©cutÃ©) - M57",
        "year_column": "exercice_comptable",
    },
    
    # =========================================================================
    # INVESTISSEMENTS (AP/CP) - Projets physiques nommÃ©s
    # =========================================================================
    "ap_projets": {
        "dataset_id": "comptes-administratifs-autorisations-de-programmes-a-partir-de-2018-m57-ville-de",
        "description": "Comptes Administratifs - AP ExÃ©cutÃ©s (projets nommÃ©s avec mission/direction)",
        "year_column": "exercice_comptable",
    },
    
    # =========================================================================
    # SUBVENTIONS - DÃ©tail des bÃ©nÃ©ficiaires
    # =========================================================================
    "subventions": {
        "dataset_id": "subventions-versees-annexe-compte-administratif-a-partir-de-2018",
        "description": "Annexe CA - Toutes subventions versÃ©es",
        "year_column": None,  # ParsÃ© depuis 'publication' ("CA 2023" â†’ 2023)
    },
    "associations": {
        "dataset_id": "subventions-associations-votees-",
        "description": "Subventions aux associations votÃ©es (avec SIRET pour gÃ©oloc)",
        "year_column": "annee_budgetaire",
    },
    
    # =========================================================================
    # DONNÃ‰ES GÃ‰OLOCALISÃ‰ES
    # =========================================================================
    "logements_sociaux": {
        "dataset_id": "logements-sociaux-finances-a-paris",
        "description": "Logements sociaux financÃ©s Ã  Paris (gÃ©olocalisÃ©s)",
        "year_column": "annee_du_financement_agrement",
        "drop_columns": ["geo_shape"],  # Colonnes complexes Ã  supprimer
    },
    
    # =========================================================================
    # BUDGET VOTÃ‰ (prÃ©visionnel) - EntitÃ© sÃ©parÃ©e du CA
    # =========================================================================
    "budget_vote": {
        "dataset_id": "budgets-votes-principaux-a-partir-de-2019-m57-ville-departement",
        "description": "Budgets VotÃ©s - Budget Principal (PrÃ©visionnel) - M57",
        "year_column": "exercice_comptable",
    },
    
    # =========================================================================
    # CONTEXTE - Ne pas sommer avec le budget
    # =========================================================================
    "marches_publics": {
        "dataset_id": "liste-des-marches-de-la-collectivite-parisienne",
        "description": "MarchÃ©s publics (ENVELOPPES PLURIANNUELLES - ne pas sommer)",
        "year_column": "annee_de_notification",
    },
}


def dataset_id_to_table_name(dataset_id: str) -> str:
    """
    Convertit un dataset_id OpenData Paris en nom de table BigQuery valide.
    Remplace les tirets par des underscores (snake_case).
    
    Exemple: "comptes-administratifs-budgets-principaux-a-partir-de-2019-m57-ville-departement"
          -> "comptes_administratifs_budgets_principaux_a_partir_de_2019_m57_ville_departement"
    """
    return dataset_id.replace("-", "_").rstrip("_")


# =============================================================================
# Utilitaires
# =============================================================================

def clean_column_name(name: str) -> str:
    """
    Nettoie un nom de colonne pour BigQuery.
    
    - Supprime les accents
    - Convertit en minuscules
    - Remplace espaces/slashes/parenthÃ¨ses par underscore
    - Collapse les underscores multiples
    """
    # Supprimer les accents
    name = unicodedata.normalize('NFD', name)
    name = ''.join(c for c in name if unicodedata.category(c) != 'Mn')
    # Minuscules
    name = name.lower()
    # Remplacer caractÃ¨res spÃ©ciaux
    name = re.sub(r'[\s/\(\)\-\.]+', '_', name)
    name = re.sub(r'[^a-z0-9_]', '_', name)
    # Collapse underscores
    name = re.sub(r'_+', '_', name)
    name = name.strip('_')
    return name


def get_dataset_info(dataset_id: str) -> dict:
    """RÃ©cupÃ¨re les mÃ©tadonnÃ©es d'un dataset."""
    url = f"{OPENDATA_API}/catalog/datasets/{dataset_id}"
    response = requests.get(url, timeout=30)
    response.raise_for_status()
    return response.json()


def get_available_years(dataset_id: str, year_column: str) -> list[int]:
    """RÃ©cupÃ¨re les annÃ©es disponibles pour un dataset."""
    url = f"{OPENDATA_API}/catalog/datasets/{dataset_id}/records"
    params = {
        "group_by": year_column,
        "limit": 50,
    }
    response = requests.get(url, params=params, timeout=30)
    response.raise_for_status()
    data = response.json()
    
    years = []
    for result in data.get("results", []):
        year_value = result.get(year_column)
        if year_value:
            # GÃ©rer les formats date vs integer
            if isinstance(year_value, str) and "T" in year_value:
                year = int(year_value[:4])
            else:
                year = int(year_value)
            years.append(year)
    
    return sorted(years, reverse=True)


def download_dataset(dataset_id: str, limit: int = -1) -> pd.DataFrame:
    """
    TÃ©lÃ©charge un dataset complet depuis l'API Paris Open Data.
    
    Utilise l'endpoint export pour Ã©viter les limites de pagination.
    """
    url = f"{OPENDATA_API}/catalog/datasets/{dataset_id}/exports/json"
    params = {"limit": limit}
    
    print(f"  ğŸ“¥ TÃ©lÃ©chargement depuis {dataset_id}...")
    response = requests.get(url, params=params, timeout=300, stream=True)
    response.raise_for_status()
    
    # Parser le JSON
    data = response.json()
    
    if not data:
        print(f"  âš ï¸ Aucune donnÃ©e trouvÃ©e")
        return pd.DataFrame()
    
    df = pd.DataFrame(data)
    print(f"  âœ“ {len(df):,} lignes tÃ©lÃ©chargÃ©es")
    
    return df


def upload_to_bigquery(
    df: pd.DataFrame,
    table_name: str,
    project_id: str = PROJECT_ID,
    dataset_id: str = DATASET_ID,
    drop_columns: list = None,
) -> int:
    """
    Upload un DataFrame vers BigQuery.
    
    CrÃ©e la table si elle n'existe pas, sinon remplace les donnÃ©es.
    
    Args:
        df: DataFrame Ã  uploader
        table_name: Nom de la table destination
        project_id: ID du projet GCP
        dataset_id: ID du dataset BigQuery
        drop_columns: Liste de colonnes Ã  supprimer (pour Ã©viter les types complexes)
    """
    if df.empty:
        print(f"  âš ï¸ DataFrame vide, skip upload")
        return 0
    
    # Nettoyer les noms de colonnes
    original_columns = df.columns.tolist()
    new_columns = {col: clean_column_name(col) for col in original_columns}
    df = df.rename(columns=new_columns)
    
    # Supprimer les colonnes problÃ©matiques (gÃ©o complexes, etc.)
    if drop_columns:
        for col in drop_columns:
            col_clean = clean_column_name(col)
            if col_clean in df.columns:
                df = df.drop(columns=[col_clean])
                print(f"  ğŸ—‘ï¸ Colonne supprimÃ©e: {col_clean} (type complexe)")
    
    # Convertir les colonnes avec des dicts/lists en strings (pour les colonnes gÃ©o restantes)
    for col in df.columns:
        if df[col].dtype == 'object':
            # VÃ©rifier si c'est une colonne avec des dicts/lists
            sample = df[col].dropna().head(1)
            if len(sample) > 0 and isinstance(sample.iloc[0], (dict, list)):
                df[col] = df[col].apply(lambda x: json.dumps(x) if isinstance(x, (dict, list)) else x)
                print(f"  ğŸ“ Colonne convertie en JSON string: {col}")
    
    # Afficher le mapping des colonnes modifiÃ©es
    print(f"  ğŸ“ Colonnes nettoyÃ©es:")
    changed = [(old, new) for old, new in new_columns.items() if old != new]
    if changed:
        for old, new in changed[:5]:  # Limiter l'affichage
            print(f"      {old} â†’ {new}")
        if len(changed) > 5:
            print(f"      ... et {len(changed) - 5} autres")
    
    # Initialiser le client BigQuery
    client = bigquery.Client(project=project_id)
    
    # CrÃ©er le dataset s'il n'existe pas
    dataset_ref = f"{project_id}.{dataset_id}"
    try:
        client.get_dataset(dataset_ref)
    except Exception:
        dataset = bigquery.Dataset(dataset_ref)
        dataset.location = "EU"
        client.create_dataset(dataset)
        print(f"  âœ“ Dataset {dataset_id} crÃ©Ã©")
    
    # Upload vers BigQuery
    table_ref = f"{project_id}.{dataset_id}.{table_name}"
    print(f"  ğŸ“¤ Upload vers {table_ref}...")
    
    job_config = bigquery.LoadJobConfig(
        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,
    )
    
    try:
        job = client.load_table_from_dataframe(df, table_ref, job_config=job_config)
        job.result()  # Attendre la fin
        
        # VÃ©rifier
        table = client.get_table(table_ref)
        print(f"  âœ“ {table.num_rows:,} lignes chargÃ©es dans {table_name}")
        return table.num_rows
    except Exception as e:
        print(f"  âŒ Erreur upload: {e}")
        # Essayer avec CSV comme fallback
        print(f"  ğŸ”„ Tentative upload via CSV...")
        try:
            import io
            csv_buffer = io.StringIO()
            df.to_csv(csv_buffer, index=False)
            csv_buffer.seek(0)
            
            job_config = bigquery.LoadJobConfig(
                write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,
                source_format=bigquery.SourceFormat.CSV,
                skip_leading_rows=1,
                autodetect=True,
            )
            job = client.load_table_from_file(csv_buffer, table_ref, job_config=job_config)
            job.result()
            
            table = client.get_table(table_ref)
            print(f"  âœ“ {table.num_rows:,} lignes chargÃ©es via CSV dans {table_name}")
            return table.num_rows
        except Exception as e2:
            print(f"  âŒ Erreur upload CSV: {e2}")
            return 0


# =============================================================================
# Synchronisation
# =============================================================================

def sync_source(source_name: str, dry_run: bool = False) -> dict:
    """
    Synchronise une source de donnÃ©es.
    
    Args:
        source_name: Nom de la source (clÃ© dans DATASETS)
        dry_run: Si True, tÃ©lÃ©charge sans uploader
    
    Returns:
        dict avec stats de synchronisation
    """
    if source_name not in DATASETS:
        raise ValueError(f"Source inconnue: {source_name}. Sources disponibles: {list(DATASETS.keys())}")
    
    config = DATASETS[source_name]
    dataset_id = config["dataset_id"]
    table_name = dataset_id_to_table_name(dataset_id)  # Nom exact OpenData en snake_case
    description = config["description"]
    year_column = config["year_column"]
    drop_columns = config.get("drop_columns", [])
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š {source_name.upper()}: {description}")
    print(f"{'='*60}")
    
    # RÃ©cupÃ©rer les infos du dataset
    try:
        info = get_dataset_info(dataset_id)
        modified = info.get("metas", {}).get("default", {}).get("modified", "N/A")
        print(f"  DerniÃ¨re MAJ: {modified}")
    except Exception as e:
        print(f"  âš ï¸ Impossible de rÃ©cupÃ©rer les mÃ©tadonnÃ©es: {e}")
    
    # AnnÃ©es disponibles
    years = []
    if year_column:
        try:
            years = get_available_years(dataset_id, year_column)
            print(f"  AnnÃ©es disponibles: {years}")
        except Exception as e:
            print(f"  âš ï¸ Impossible de rÃ©cupÃ©rer les annÃ©es: {e}")
    else:
        print(f"  â„¹ï¸ Pas de colonne annÃ©e (sera parsÃ© depuis 'publication')")
    
    # TÃ©lÃ©charger
    df = download_dataset(dataset_id)
    
    if df.empty:
        return {"source": source_name, "rows": 0, "status": "empty"}
    
    # Upload (sauf dry_run)
    if dry_run:
        print(f"  ğŸ”¸ DRY RUN - pas d'upload")
        rows = len(df)
    else:
        rows = upload_to_bigquery(df, table_name, drop_columns=drop_columns)
    
    return {
        "source": source_name,
        "table": table_name,
        "rows": rows,
        "years": years,
        "status": "success",
    }


def sync_all(dry_run: bool = False) -> list[dict]:
    """Synchronise toutes les sources."""
    results = []
    
    print("\n" + "="*60)
    print("ğŸ”„ SYNCHRONISATION PARIS OPEN DATA â†’ BIGQUERY")
    print("="*60)
    print(f"  Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"  Projet: {PROJECT_ID}")
    print(f"  Dataset: {DATASET_ID}")
    if dry_run:
        print("  âš ï¸ MODE DRY RUN - pas d'upload")
    
    for source_name in DATASETS.keys():
        try:
            result = sync_source(source_name, dry_run=dry_run)
            results.append(result)
        except Exception as e:
            print(f"  âŒ Erreur pour {source_name}: {e}")
            results.append({
                "source": source_name,
                "rows": 0,
                "status": "error",
                "error": str(e),
            })
    
    # RÃ©sumÃ©
    print("\n" + "="*60)
    print("ğŸ“‹ RÃ‰SUMÃ‰ DE LA SYNCHRONISATION")
    print("="*60)
    
    total_rows = 0
    for r in results:
        status_icon = "âœ“" if r["status"] == "success" else "âŒ" if r["status"] == "error" else "â—‹"
        rows = r.get("rows", 0)
        total_rows += rows
        years = r.get("years", [])
        years_str = f" ({min(years)}-{max(years)})" if years else ""
        print(f"  {status_icon} {r['source']}: {rows:,} lignes{years_str}")
    
    print(f"\n  TOTAL: {total_rows:,} lignes")
    
    return results


def list_sources():
    """Liste les sources disponibles avec leurs infos."""
    print("\n" + "="*60)
    print("ğŸ“‹ SOURCES DISPONIBLES")
    print("="*60)
    
    for name, config in DATASETS.items():
        table_name = dataset_id_to_table_name(config['dataset_id'])
        print(f"\n  {name}")
        print(f"    Description: {config['description']}")
        print(f"    Dataset ID: {config['dataset_id']}")
        print(f"    Table BQ: raw.{table_name}")
        
        # RÃ©cupÃ©rer les annÃ©es si possible
        try:
            years = get_available_years(config["dataset_id"], config["year_column"])
            print(f"    AnnÃ©es: {min(years)}-{max(years)} ({len(years)} annÃ©es)")
        except Exception:
            print(f"    AnnÃ©es: (impossible Ã  rÃ©cupÃ©rer)")


def check_data_availability() -> dict:
    """
    VÃ©rifie la disponibilitÃ© des donnÃ©es par annÃ©e.
    
    Retourne un dict avec le statut de chaque annÃ©e:
    - COMPLET: toutes les sources disponibles
    - PARTIEL: certaines sources manquantes
    - BUDGET_SEUL: uniquement le budget principal
    """
    print("\n" + "="*60)
    print("ğŸ” VÃ‰RIFICATION DISPONIBILITÃ‰ DES DONNÃ‰ES")
    print("="*60)
    
    # Collecter les annÃ©es par source
    years_by_source = {}
    for name, config in DATASETS.items():
        year_column = config.get("year_column")
        if not year_column:
            print(f"  {name}: (annÃ©e non disponible - parsÃ© depuis publication)")
            years_by_source[name] = set()
            continue
        try:
            years = get_available_years(config["dataset_id"], year_column)
            years_by_source[name] = set(years)
            print(f"  {name}: {min(years)}-{max(years)}")
        except Exception as e:
            print(f"  {name}: âŒ {e}")
            years_by_source[name] = set()
    
    # DÃ©terminer le statut par annÃ©e
    all_years = set()
    for years in years_by_source.values():
        all_years.update(years)
    
    availability = {}
    for year in sorted(all_years, reverse=True):
        has_budget = year in years_by_source.get("budget_execute", set())
        has_subventions = year in years_by_source.get("associations", set())
        has_autorisations = year in years_by_source.get("ap_execute", set())
        has_arrondissements = year in years_by_source.get("arrondissements", set())
        
        if has_budget and has_subventions and has_autorisations and has_arrondissements:
            status = "COMPLET"
        elif has_budget and has_subventions:
            status = "PARTIEL"
        elif has_budget:
            status = "BUDGET_SEUL"
        else:
            status = "INCOMPLET"
        
        availability[year] = {
            "status": status,
            "has_budget": has_budget,
            "has_subventions": has_subventions,
            "has_autorisations": has_autorisations,
            "has_arrondissements": has_arrondissements,
        }
    
    # Afficher le rÃ©sumÃ©
    print("\n  AnnÃ©e   â”‚ Budget â”‚ Subv â”‚ AP/CP â”‚ Arr. â”‚ Statut")
    print("  â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€")
    for year in sorted(availability.keys(), reverse=True):
        a = availability[year]
        b = "âœ“" if a["has_budget"] else "âœ—"
        s = "âœ“" if a["has_subventions"] else "âœ—"
        ap = "âœ“" if a["has_autorisations"] else "âœ—"
        ar = "âœ“" if a["has_arrondissements"] else "âœ—"
        print(f"  {year}    â”‚   {b}    â”‚  {s}   â”‚   {ap}   â”‚  {ar}   â”‚ {a['status']}")
    
    return availability


# =============================================================================
# Main
# =============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="Synchronisation Paris Open Data â†’ BigQuery",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples:
    # Synchroniser toutes les sources
    python scripts/sync_opendata.py
    
    # Synchroniser une source spÃ©cifique
    python scripts/sync_opendata.py --source budget_execute
    
    # Mode dry run (tÃ©lÃ©charge sans uploader)
    python scripts/sync_opendata.py --dry-run
    
    # VÃ©rifier la disponibilitÃ© des donnÃ©es
    python scripts/sync_opendata.py --check
        """
    )
    parser.add_argument(
        "--source",
        choices=list(DATASETS.keys()),
        help="Source spÃ©cifique Ã  synchroniser"
    )
    parser.add_argument(
        "--list",
        action="store_true",
        help="Lister les sources disponibles"
    )
    parser.add_argument(
        "--check",
        action="store_true",
        help="VÃ©rifier la disponibilitÃ© des donnÃ©es par annÃ©e"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="TÃ©lÃ©charger sans uploader vers BigQuery"
    )
    
    args = parser.parse_args()
    
    if args.list:
        list_sources()
        return
    
    if args.check:
        check_data_availability()
        return
    
    if args.source:
        sync_source(args.source, dry_run=args.dry_run)
    else:
        sync_all(dry_run=args.dry_run)
    
    print("\nâœ… Synchronisation terminÃ©e !")
    print("\nProchaines Ã©tapes:")
    print("  1. dbt run                           # Transformer les donnÃ©es")
    print("  2. python scripts/enrich_geo_data.py # Enrichir via LLM")
    print("  3. python scripts/export_sankey_data.py # Exporter pour frontend")


if __name__ == "__main__":
    main()
